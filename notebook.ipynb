{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opts Scatter Curve [width=600 height=400 show_grid=True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python + processamento paralelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento padrão\n",
    "(single core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar cpfs falsos\n",
    "cpfs = np.random.randint(1, 999_999_999, size=(5_000_000, ))\n",
    "cpfs = list(map(str, cpfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysha(cpf: str):\n",
    "    \"\"\"\n",
    "    Essa função calcula o hash de um CPF\n",
    "    Ela adiciona um sal, fixo como 'aaaaa'\n",
    "    Também adiciona uma pimenta, a string\n",
    "    de um número aleatório entre 1000 e 9999\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cpf: str\n",
    "        cpf, ja como string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        string hexdigest do hash + salt + pepper\n",
    "    \"\"\"\n",
    "    salt = 'aaaaa'\n",
    "    pepper = str(random.randint(1000, 9999))\n",
    "    cpf = salt + cpf + pepper\n",
    "    h = hashlib.sha256(cpf.encode('utf8')).hexdigest()\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num de cpfs: {len(cpfs):,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Benchmarking\n",
    "O tempo de casa uma das células abaixo, no meu computador é de, aprox, 12 segundos  \n",
    "O python esta rodando cada um dos 5mm de cpfs um após o outro. Num núcleo somente (confira no htop ou gerenciador de tarefas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in cpfs:\n",
    "    _ = mysha(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = [mysha(i) for i in cpfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = list(map(mysha, cpfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clockit(cpfs: list, n: int):\n",
    "    \"\"\"\n",
    "    Retorna o tempo necessário para calcular o hash de todos os cpfs numa lista\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cpfs: list\n",
    "        lista de cpfs (que são strings)\n",
    "\n",
    "    n: int\n",
    "        número de cpfs que será calculado\n",
    "        `n` deve ser menor que o tamanho da lista\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        o número de segundos necessário para o cálculo\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    _ = [mysha(i) for i in cpfs[:n]]\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "aux = []\n",
    "n = 1\n",
    "while 1:\n",
    "    aux.append((n, clockit(cpfs, n)))\n",
    "    n *= 2\n",
    "    if n > 500_000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Curve(aux, kdims='n', vdims='tempo', label='Complexidade linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Porque so utilizamos 1 núcleo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação padrão do python (CPython) transforma o código em python para byte code.  \n",
    "O byte code é interpretado pelo CPython e transformado em assembly para ser executado pelo processador.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de interpretação do byte code pelo CPython **não** é [thread-safe](https://en.wikipedia.org/wiki/Thread_safety).  \n",
    "Para contornar isso, foi criado o Global Interpreter Locker (vulgo, GIL) (mais info [aqui](https://wiki.python.org/moin/GlobalInterpreterLock), [aqui](https://docs.python.org/3/c-api/init.html#thread-state-and-the-global-interpreter-lock), [aqui](https://realpython.com/python-gil/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, uma das consquencias do GIL é que o python não consegue ser executado em vários núcleos de forma nativa.  \n",
    "Então foram criadas alternativas para aplicações que são limitadas pelo processamento (cpu bound)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing - uma biblioteca que você ja tem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: neste notebook, de vez em quando, é repetida a importação das libs.  \n",
    "Vc não precisa reimportar novamente, mas como o notebook é muito longo, esse código repetido facilita vc rodar so um pedaço caso precise reiniciar a kernel (em caso de estouro de memória, por exemplo).  \n",
    "No jupyter notebook padrão, o atalho para reiniciar a kernel é `0+0` (zero), para interromper uma célula que esta rodando: `i+i`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import time\n",
    "import random\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Gerar cpfs falsos\n",
    "cpfs = np.random.randint(1, 999_999_999, size=(5_000_000, ))\n",
    "cpfs = list(map(str, cpfs))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mysha(cpf):\n",
    "    salt = 'aaaaa'\n",
    "    pepper = str(random.randint(1000, 9999))\n",
    "    cpf = salt + cpf + pepper\n",
    "    h = hashlib.sha256(cpf.encode('utf8')).hexdigest()\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta célula cria um pool com o número de cpus que o python enxerga no seu computador\n",
    "# caso queira um número diferente, basta colocar o número que quiser no lugar do `mp.cpu_count()`\n",
    "pool = mp.Pool(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = pool.map(mysha, cpfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing é uma lib que ja vem instalada por padrão com o python (built-in).  \n",
    "Ela tem objetos convenientes que facilitam o processamento paralelo simples  \n",
    "São criados vários interpretadores, um para cada processo. Eles não compartilham memória. Muitas vezes a comunicação entre eles é um gargalo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: toda vez que vier uma barra como a abaixo, é uma indicação para reiniciar a kernel para liberar memória  \n",
    "Você não precisa fazer isso, entretanto. Mas pode ser util para não usar o swap ou arquivo de paginação  \n",
    "Se não o fizer, lembre de não instanciar dois clientes Dask  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra o Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask é uma biblioteca construída junto a outros projetos python (numpy, pandas, sklearn...) para melhorar a escalabilidade de todo o ecossistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Porque não o spark?!\n",
    "O spark é uma excelente ferramenta. Foi construído em cima do hadoop, existe há muitos anos no mercado e tem muitos profissionais que sabem utilizar  \n",
    "Funciona muito bem. Qualquer um pode utilizar o spark sem problemas  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas ó spark, feito há mais tempo, foi construido para funcionar muito bem em volta do ecossistema do hadoop. Não foi feito pensando no ecossistema, ainda incipiente, do python.  \n",
    "Ele é foi construído num nível mais alto. Aplicando otimizações em computações padrões. Basicamente um framework para mapple-shuffle-reduce distribuídos.  \n",
    "É dificil implementar outras coisas que dependam de mais flexbilidade.  \n",
    "\n",
    "O dask é mais \"low-level\".  A implementação do agendador de tarefas é a base em cima da qual é construído.  \n",
    "Ele da a flexibilidade de construir o que quiser em cima dele. Permitiria, por exemplo, que fosse construído o proprio Spark em cima do dask.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra diferença importante é que o dask é feito em python e para python. Ele também é desenvolvido em conjunto com outras libs comuns na área de dados (numpy, pandas, sklearn, etc).  \n",
    "Isso facilita, por exemplo, que seja utilizado por qualquer pessoa com intimidade com o Pandas, sem ter que reaprender sobre o spark RDD, pyspark ou qualquer outra coisa diferente.  \n",
    "Assim a gente foca em analisar dados e ao invés de ficar aprendendo a utilizar ferramentas ou linguagens novas  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma comparação mais detalhada sobre Dask e Spark [aqui](https://docs.dask.org/en/latest/spark.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### O que tem dentro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dask é composto de duas partes:  \n",
    "- um scheduler de tarefas\n",
    "- uma coleção de estruturas de dados distribuidas, como o dask dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dask-scheduler-overview.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### O scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"grid_search_schedule.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No grafo de tarefas, cada nó é uma tarefa que pode ser executada por um worker diferente  \n",
    "Os vertices são a comunicação entre os workers  \n",
    "Os nós vermelhos são os que estão sendo processados no momento, enquanto os azuis são os já concluídos  \n",
    "O dask avalia os calculos de forma \"lazy\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk is cheap, show me the code\n",
    "(Linus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "# aqui a gente instancia um cliente para o dask.\n",
    "# Este estara rodando localmente somente, mas pode ser conectador a outros computadores\n",
    "client = Client(threads_per_worker=4, n_workers=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Um exemplo simples do dask funcionando em paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(0.5)\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    time.sleep(0.5)\n",
    "    return x - 1\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(0.5)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "inc = dask.delayed(inc)\n",
    "dec = dask.delayed(dec)\n",
    "add = dask.delayed(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)\n",
    "# A lembrar que `z` será um objeto do tipo `Delayed`, não mais uma função\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: o gráfico acima é gerado pela lib graphviz. Ela tem dependencias de sistema.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As estruturas de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das maiores vantagens do dask é poder trabalhar em escala com a mesma API que ja estamos acostumados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.Sex.replace({'female': 0, 'male': 1}, inplace=True)\n",
    "titanic.drop(columns=['Name', 'Ticket', 'Cabin', 'Embarked'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Quantidade de linhas: {titanic.shape[0]:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "titanic.groupby('Sex').Survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "titanic[['Age', 'Fare']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é um exemplo bem \"feliz\". Um hello world na área de dados..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"titanic-kaggle.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### construindo um dataset maior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: a célula abaixo criar um arquivo e salva no seu computador. Você so precisa executar ela uma vez.  \n",
    "Depois pode comentar a célula inteira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota 2: a célular abaixo cria um dataset 250k vezes maior que o titanic normal.  \n",
    "Foi dimensionada para uma máquina de 100GB de memória ram.  \n",
    "Caso esteja executando num computador menor, reduza o fator de 250_000, em 10x ou 20x (como achar conveniente)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execute essa célula na primeira vez somente\n",
    "titanic_raiz = pd.concat([titanic] * 250_000)\n",
    "titanic_raiz.to_parquet('titanic.parquet', index=False, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raiz = pd.read_parquet('titanic.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Quantidade de linhas: {titanic_raiz.shape[0]:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're talking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"titanic-raiz.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "titanic_raiz.groupby('Sex').Survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "titanic_raiz[['Age', 'Fare']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: caso você tenha feito a conta, notou que o tempo das operações não aumentou linearmente.  \n",
    "De fato, o pandas utiliza o numpy q tem algumas estruturas de dados e algorítimos que são mais eficientes do que O(n).  \n",
    "Mas, ainda assim, vc fica limitado pela quantidade de memória e, quando o dataset for grande o suficiente, pelo processamento num único núcleo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reparticionando o dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: assim como acima, as próximas 8 células criam um arquivo (na verdade vários) no seu computador na primeira vez que forem executadas.  \n",
    "Depois você pode comenta-las  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "    n_workers=1,\n",
    "    threads_per_worker=1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('titanic.parquet', columns=['Pclass', 'Sex', 'Age', 'Fare', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()\n",
    "progress(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(npartitions=16)\n",
    "progress(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('./titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Agora vai!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "    n_workers=1,\n",
    "    threads_per_worker=16,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quando ha vários arquivos, o dask ja lê eles particionados\n",
    "df = dd.read_parquet('titanic/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = df.groupby('Sex').Survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.persist()\n",
    "progress(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com execução 'lazy', o arquivo so é lido qnd é necessário.  \n",
    "Não só isso, como o arquivo salvo é colunar ([.parquet](https://parquet.apache.org/)), so são lidas as colunas que forem necessárias.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o método .compute() retorna um objeto único (neste caso um pd.Serie), não mais so um Delayed\n",
    "x.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = df[['Age', 'Fare']].describe()\n",
    "x = x.persist()\n",
    "progress(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.groupby(by=[\n",
    "    df['Age'].map_partitions(pd.cut, 10),\n",
    "    'Pclass',\n",
    "    'Sex'\n",
    "]).Survived.mean().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chega de titanic..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"titanic-big-data.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "    n_workers=1,\n",
    "    threads_per_worker=16,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxis = dd.read_csv(\n",
    "    's3://nyc-tlc/trip data/yellow_tripdata_2018-*.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: o dask utiliza s3fs (que utiliza o boto) para ler arquivos diretamente do S3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxis = taxis.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tamanho do dataset\n",
    "taxis.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantos passageiros\n",
    "taxis.passenger_count.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantidade de corridas por número de passageiros\n",
    "x = taxis.groupby('passenger_count').size().compute()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = taxis[(taxis.tip_amount > 0) & (taxis.fare_amount > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['tip_pct'] = df2.tip_amount / df2.fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.tpep_pickup_datetime = df2.tpep_pickup_datetime.astype('M8[us]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hora = df2.groupby(df2.tpep_pickup_datetime.dt.hour)\\\n",
    "    .tip_pct.mean().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hora = hora.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Curve(hora.values, kdims='Hora', vdims='Gorjeta (%)').opts(width=500, height=300, show_grid=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
